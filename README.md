# English to Hindi Translation using Transformer from scratch
 

## Overview
This repository contains a full implementation of a **Transformer-based Neural Machine Translation (NMT) model** trained from scratch for **English-to-Hindi translation**. The model is implemented using **PyTorch** and follows the architecture outlined in the **"Attention Is All You Need"** paper.

## Features
тЬЕ Implements **Self-Attention, Multi-Head Attention, and Positional Encoding** from scratch.
тЬЕ Uses **custom tokenizers** trained on the dataset.
тЬЕ Supports **CUDA acceleration** for faster training.
тЬЕ Evaluates using **greedy decoding** for predictions.
тЬЕ Provides **training & validation accuracy/loss tracking**.

---

## Training Details
ЁЯЦея╕П **Training Device:** CUDA
ЁЯУК **Train Dataset Size:** 1,659,083
ЁЯУК **Validation Dataset Size:** 520
тП│ **Total Training Time:** **74,026.96 seconds (~20.56 hours)**
ЁЯУИ **Total Epochs:** 10

### Epoch Training Times:
```
Epoch 0 took 7221.75 seconds.
Epoch 1 took 7276.88 seconds.
Epoch 2 took 7248.24 seconds.
Epoch 3 took 7285.44 seconds.
Epoch 4 took 7276.41 seconds.
Epoch 5 took 7163.37 seconds.
Epoch 6 took 7117.83 seconds.
Epoch 7 took 7223.12 seconds.
Epoch 8 took 7441.33 seconds.
Epoch 9 took 7971.59 seconds.
```

---

## Model Architecture
The model follows the **Transformer** architecture with the following components:
- **Input Embedding Layer**: Converts token indices to dense vectors.
- **Positional Encoding**: Adds positional information to embeddings.
- **Encoder & Decoder Layers**: Stacks of self-attention and feed-forward layers.
- **Multi-Head Attention Mechanisms**: Improves contextual understanding.
- **Projection Layer**: Outputs probabilities over the vocabulary.

---



## Usage
### Training
To train the model, run:
```bash
$ python train.py
```

### Sample Prediction
| Source | Expected | Predicted |
|--------|---------|-----------|
| NHPS officials and staff were working hard to reinstate the power supply. | рдПрдирдПрдЪрдкреАрд╕реА рдХреЗ рдЕрдзрд┐рдХрд╛рд░реА рд╡ рдХрд░реНрдордЪрд╛рд░реА рддреАрди рджрд┐рди рд╕реЗ рд▓рдЧрд╛рддрд╛рд░ рдмрд┐рдЬрд▓реА рд╕реЗрд╡рд╛ рдмрд╣рд╛рд▓ рдХрд░рдиреЗ рдореЗрдВ рд▓рдЧреЗ рд╣реБрдП рдереЗред | рдмрд┐рдЬрд▓реА рдЖрдкреВрд░реНрддрд┐ рдХреЛ рд░реЛрдХрдиреЗ рдХреЗ рд▓рд┐рдП рдЕрдзрд┐рдХрд╛рд░рд┐рдпреЛрдВ рдФрд░ рдХрд░реНрдордЪрд╛рд░рд┐рдпреЛрдВ рдХреЛ рдХрдареЛрд░ рдкрд░рд┐рд╢реНрд░рдо рдХрд░рдирд╛ рдкрдбрд╝рддрд╛ рдерд╛ред |

---

## Next Steps
ЁЯФ╣ Improve translation accuracy using **beam search decoding**.
ЁЯФ╣ Fine-tune model on **larger, domain-specific datasets**.
ЁЯФ╣ Optimize tokenization with **subword embeddings (BPE)**.


Would love feedback and suggestions! ЁЯЪАЁЯФе

#DeepLearning #NLP #Transformers #MachineTranslation #AI
